\documentclass{article}
\usepackage{enumerate}
\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{enumerate}

\item

<<>>=
RENTS <- read.csv("~/Documents/Stat103/OFFICE RENTS.csv")
lmRENTS<-lm(RENTS[,2]~RENTS[,1])
summary(lmRENTS)
#plot(RENTS[,2], RENTS[,1])
#abline(lm(RENTS[,2] ~ RENTS[,1]))

@


\begin{enumerate}[a]
\item y=20.64-0.30

\item Yes, because the p value of the negative slope is 0.00209 which is far below 0.05

\item Since the coefficient of determination is .2911, it appears the data fits fairly poorly. Only 29.11\% of the variation in Y is explained by the model.

\item The coefficient $\hat{\beta} _1$ represents the estimated slope of the regression line. For every one percent increase in vacancy we predict a mean decrease of $\hat{\beta} _1$ = 0.30 increase in dollars per square foot of rent. 

\item 
Typically we cannot interpret $\hat{\beta} _0$ because we do no typically measure the predictor at 0, thus the value of this intercept is extrapolated. If we do measure at or very close to x=0, then we may interpret $\hat{\beta} _0$ as $\hat{y}$ when x=0. In this example interpreting $\hat{\beta} _0$ would be extrapolation.

\item Prior to using this model we must check the two assumptions made by the model. The first is that the error, $\epsilon$ is normally distributed with mean 0 and constant standard deviation. We also assumed that the residuals are independent of eachother

\item

<<>>=
RentValue <- RENTS$Rent
Vacancy <- RENTS$Vacancy
value<-data.frame(Vacancy = 12)
predict(lm(RentValue~Vacancy), value, level = .90, interval = "confidence")
@
So we may predict with 90\% confidence the the average price of rent in a city with a 12\% vacancy rate is between 16.1 dollars per square foot and 17.9 dollars per square foot.
\end{enumerate}
\item

\begin{enumerate}[a]
\item 
<<>>=
FordTaurus <- read.csv("~/Documents/Stat103/FordTaurus.csv")
Price <- FordTaurus$Price
Odometer <- FordTaurus$Odometer
lmFORD <- lm(Price~Odometer)
summary(lmFORD)
plot(Odometer,Price)
abline(lmFORD)
@



There does appear to be a linear relationship between odometer reading and resale price. The t-value of the slope is very small, -13.49, indicating the slope is not zero. There appears to be a strong linear relationship because the correlation coefficient is 65.01$\%$

\item 

$\hat{y}$ = 6533 - 0.0311*x

\item The two assumptions about the error variable are the $\epsilon$ is normally distributed with mean 0 and constant standard deviation, and the residuals are independent.

\item
<<fig=TRUE>>=
residualsFORD<-residuals(lmFORD)
shapiro.test(residualsFORD)
plot(lmFORD, which = 1)
@

I would determine the assumptions in part c are satisfied. Since the null hypothesis of the Shapiro-Wilk test for normality is that the residuals are normally distributed, a high p-value of 0.812 indicated that it is very likely the residuals are normally distributed. Upon inspecting the plot of the fitted y values and the residuals, it appears the variance of the residuals is more or less constant. There is a little tapering to the right, but is does not seem significant. 

\item From the multiple r-squared we see that 65.01\% of the variation in the resale price is explained by the regression. 

\item The coefficient $\hat{\beta} _0$=6533 extrapolates that a three year old Ford Taurus that has driven 0 miles will be resold for \$6533, but we do not interpret this because it is extrapolation. The coefficient $\hat{\beta} _1$=-0.03116 indicated that for every 100 additional miles on the odometer, we predict an average decrease of \$3.12 in resale price.

\item

$\indent$ i)The numberical value of $S_{\epsilon}$ for the model is 151.6

$\indent$ ii)The proper units for $S_{\epsilon}$ are dollars

$\indent$ iii) We interpret $S_{\epsilon}$ as the estimator for the standard deviation of the residuals. 

\item 

<<>>=
car <- data.frame(Odometer = 36000)
predict(lmFORD, car, interval = "confidence", level = .95)
@
I would expect, with 95\% confidence to sell my Taurus with 36,000 miles on it for between \$5382 and \$5442.

\item

<<fig=TRUE>>=
plot<-boxplot.default(FordTaurus)
plot$out
@

<<fig=TRUE>>=
plot(lmFORD,which=5)
@

<<fig=TRUE>>=
plot(lmFORD,which = 4)
@

I have not figured out how to find which points actuall have high leverage with R.
The influeantial point by box plot is row 8, ODO 19057, Price \$5939.61.
The potential outliers are rows 14, 19, and 78
\item

Leverage is the measure of how influential a single point is on the regression line. It is measured for each point by removing the point and observing how much it changes the slope of the regression line. Finding points with high leverage is important for finding potential outliers that lie close to the regression line.
\end{enumerate}

\item

$H_0$: $\beta _1$ = 0

$H_A$: $\mu$ $\neq$ 0

We conclude X and Y are not linearly related if we fail to reject $H_0$.
\end{enumerate}
\end{document}